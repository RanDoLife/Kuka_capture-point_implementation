#!/usr/bin/env python3
# Python 2/3 compatibility imports
from __future__ import print_function
from six.moves import input

import sys
import copy
import rospy
import moveit_commander
import moveit_msgs.msg
import geometry_msgs.msg
from kuka_rsi_hw_interface.srv import write_outputs

try:
    from math import pi, tau, dist, fabs, cos
except:  # For Python 2 compatibility
    from math import pi, fabs, cos, sqrt

    tau = 2.0 * pi

    def dist(p, q):
        return sqrt(sum((p_i - q_i) ** 2.0 for p_i, q_i in zip(p, q)))


from std_msgs.msg import String
from moveit_commander.conversions import pose_to_list
import cv2
import numpy as np
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from scipy.spatial.transform import Rotation as R

# Transform a given input pose from one fixed frame to another
import tf2_ros
import tf2_geometry_msgs  # **Do not use geometry_msgs. Use this instead for PoseStamped

DEFAULT_SPEED_FACTOR = 0.01
DEFAULT_ACCEL_FACTOR = 0.01

def all_close(goal, actual, tolerance):
    """
    Convenience method for testing if the values in two lists are within a tolerance of each other.
    For Pose and PoseStamped inputs, the angle between the two quaternions is compared (the angle
    between the identical orientations q and -q is calculated correctly).
    @param: goal       A list of floats, a Pose or a PoseStamped
    @param: actual     A list of floats, a Pose or a PoseStamped
    @param: tolerance  A float
    @returns: bool
    """
    if type(goal) is list:
        for index in range(len(goal)):
            if abs(actual[index] - goal[index]) > tolerance:
                return False

    elif type(goal) is geometry_msgs.msg.PoseStamped:
        return all_close(goal.pose, actual.pose, tolerance)

    elif type(goal) is geometry_msgs.msg.Pose:
        x0, y0, z0, qx0, qy0, qz0, qw0 = pose_to_list(actual)
        x1, y1, z1, qx1, qy1, qz1, qw1 = pose_to_list(goal)
        # Euclidean distance
        d = dist((x1, y1, z1), (x0, y0, z0))
        # phi = angle between orientations
        cos_phi_half = fabs(qx0 * qx1 + qy0 * qy1 + qz0 * qz1 + qw0 * qw1)
        return d <= tolerance and cos_phi_half >= cos(tolerance / 2.0)

    return True

def transform_pose(input_pose, from_frame, to_frame):

    # **Assuming /tf2 topic is being broadcasted
    tf_buffer = tf2_ros.Buffer()
    listener = tf2_ros.TransformListener(tf_buffer)

    pose_stamped = tf2_geometry_msgs.PoseStamped()
    pose_stamped.pose = input_pose
    pose_stamped.header.frame_id = from_frame
    pose_stamped.header.stamp = rospy.Time.now()

    try:
        # ** It is important to wait for the listener to start listening. Hence the rospy.Duration(1)
        output_pose_stamped = tf_buffer.transform(pose_stamped, to_frame, rospy.Duration(1))
        return output_pose_stamped.pose

    except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):
        raise

class MoveGroupPythonInterface(object):
    """MoveGroupPythonInterface"""

    def __init__(self):
        super(MoveGroupPythonInterface, self).__init__()

        ## First initialize `moveit_commander`_ and a `rospy`_ node:
        moveit_commander.roscpp_initialize(sys.argv)
        rospy.init_node("move_group_python_interface", anonymous=True)

        rospy.wait_for_service('/kuka_hardware_interface/write_digital_outputs')
        self.gripper_srv = rospy.ServiceProxy('/kuka_hardware_interface/write_digital_outputs', write_outputs)
        
        ## Instantiate a `RobotCommander`_ object. Provides information such as the robot's
        ## kinematic model and the robot's current joint states
        robot = moveit_commander.RobotCommander()

        ## Instantiate a `PlanningSceneInterface`_ object.  This provides a remote interface
        ## for getting, setting, and updating the robot's internal understanding of the
        ## surrounding world:
        scene = moveit_commander.PlanningSceneInterface()

        ## Instantiate a `MoveGroupCommander`_ object.  This object is an interface
        ## to a planning group (group of joints).  In this tutorial the group is the primary
        ## arm joints in the Panda robot, so we set the group's name to "panda_arm".
        ## If you are using a different robot, change this value to the name of your robot
        ## arm planning group.
        ## This interface can be used to plan and execute motions:
        group_name = "manipulator"
        move_group = moveit_commander.MoveGroupCommander(group_name)

        ## Create a `DisplayTrajectory`_ ROS publisher which is used to display
        ## trajectories in Rviz:
        display_trajectory_publisher = rospy.Publisher(
            "/move_group/display_planned_path",
            moveit_msgs.msg.DisplayTrajectory,
            queue_size=20,
        )

        ## END_SUB_TUTORIAL

        ## BEGIN_SUB_TUTORIAL basic_info
        ##
        ## Getting Basic Information
        ## ^^^^^^^^^^^^^^^^^^^^^^^^^
        # We can get the name of the reference frame for this robot:
        planning_frame = move_group.get_planning_frame()
        print("============ Planning frame: %s" % planning_frame)

        # We can also print the name of the end-effector link for this group:
        eef_link = move_group.get_end_effector_link()
        print("============ End effector link: %s" % eef_link)

        # We can get a list of all the groups in the robot:
        group_names = robot.get_group_names()
        print("============ Available Planning Groups:", robot.get_group_names())

        # Sometimes for debugging it is useful to print the entire state of the
        # robot:
        print("============ Printing robot state")
        print(robot.get_current_state())
        print("")
        ## END_SUB_TUTORIAL

        # Misc variables
        self.robot = robot
        self.scene = scene
        self.move_group = move_group
        self.display_trajectory_publisher = display_trajectory_publisher
        self.planning_frame = planning_frame
        self.eef_link = eef_link
        self.group_names = group_names
        self.box_name=''
        
        self.move_group.set_max_velocity_scaling_factor(DEFAULT_SPEED_FACTOR)
        self.move_group.set_max_acceleration_scaling_factor(DEFAULT_ACCEL_FACTOR)
        
        # Initialize CV Bridge
        self.bridge = CvBridge()
        
        # Camera calibration parameters from Colab code
        self.mtx = np.array([
            [2554.307419, 0.000000, 681.336125],
            [0.000000, 2557.119597, 597.850967],
            [0.000000, 0.000000, 1.000000]
        ], dtype=np.float32)
        
        self.dist = np.array([-0.046462, 0.284386, 0.003340, -0.001596, 0.000000], dtype=np.float32)
        
        # Define physical marker sizes (meters)
        self.marker_physical_sizes_m = {
            3: 0.10,  # Calibration marker (10 cm)
            8: 0.04  # Target marker (3.7 cm)
        }
        
        # Define transformation matrix from OpenCV to user frame
        self.R_opencv_to_user_frame = np.array([
            [0, 0, 1],   # X_user = 0*X_o + 0*Y_o + 1*Z_o
            [-1, 0, 0],  # Y_user = -1*X_o + 0*Y_o + 0*Z_o
            [0, 1, 0]    # Z_user = 0*X_o + 1*Y_o + 0*Z_o
        ], dtype=np.float32)
        
        # Position of calibration marker (ID 3) relative to robot base (in user frame)
        self.t_robot_base_to_marker3_user_frame = np.array([-0.162, -0.274, 0.0025])
        
        # Subscribe to camera topic
        self.image_sub = rospy.Subscriber("/pylon_camera_node/image_raw", Image, self.image_callback)
        self.latest_image = None
        self.image_received = False

    def image_callback(self, msg):
        """Callback function for camera image subscription"""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.latest_image = cv_image
            self.image_received = True
        except Exception as e:
            rospy.logerr("Error converting ROS image to OpenCV: %s" % str(e))

    def get_marker_position(self):
        """Get position of target marker (ID 7) relative to robot base"""
        if not self.image_received:
            rospy.logerr("No image received from camera!")
            return None, None, None
        
        # Make a copy of the latest image to avoid conflicts with the callback
        img = self.latest_image.copy()
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # OpenCV 4.2 compatible ArUco detection (for ROS Noetic)
        aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_5X5_250)
        parameters = cv2.aruco.DetectorParameters_create()
        
        # Detect markers
        corners, ids, rejected = cv2.aruco.detectMarkers(gray_img, aruco_dict, parameters=parameters)
        
        if ids is None or len(ids) == 0:
            rospy.logerr("No ArUco markers detected!")
            return None, None, None
        
        # Flatten ids array for compatibility
        ids = ids.flatten()
        
        rospy.loginfo(f"Detected {len(ids)} ArUco markers: {ids.tolist()}")
        
        # Object points for a square marker (centered at origin)
        obj_points_single = np.array([[-0.5, 0.5, 0], [0.5, 0.5, 0], [0.5, -0.5, 0], [-0.5, -0.5, 0]], dtype=np.float32)
        
        rvecs = {}
        tvecs = {}
        
        # Detect poses for all markers
        for i in range(len(ids)):
            marker_id = ids[i]
            if marker_id not in self.marker_physical_sizes_m:
                rospy.logwarn(f"Marker ID {marker_id} not in known sizes. Skipping pose estimation.")
                continue
            
            marker_length = self.marker_physical_sizes_m[marker_id]
            scaled_obj_points = obj_points_single * marker_length
            img_pts = corners[i].reshape(4, 2).astype(np.float32)
            
            success, rvec, tvec = cv2.solvePnP(
                scaled_obj_points,
                img_pts,
                self.mtx,
                self.dist,
                flags=cv2.SOLVEPNP_IPPE_SQUARE
            )
            
            if success:
                rvecs[marker_id] = rvec
                tvecs[marker_id] = tvec
                distance = np.linalg.norm(tvec)
                rospy.loginfo(f"Marker ID {marker_id}: Distance = {distance:.3f} m")
            else:
                rospy.logerr(f"Failed to estimate pose for marker ID {marker_id}")
        
        # Check if we have the calibration marker (ID 3)
        if 3 not in tvecs:
            rospy.logerr("Calibration marker (ID 3) not found!")
            return None, None, None
        
        # Check if we have the target marker (ID 7)
        if 8 not in tvecs:
            rospy.logerr("Target marker (ID 8) not found!")
            return None, None, None
        
        # Get pose of calibration marker (ID 3) in OpenCV camera frame
        rvec_cam_opencv_to_marker3 = rvecs[3]
        tvec_cam_opencv_to_marker3 = tvecs[3]
        
        # Convert rotation vector to rotation matrix
        R_cam_opencv_to_marker3, _ = cv2.Rodrigues(rvec_cam_opencv_to_marker3)
        
        # Create homogeneous transformation matrix: camera_opencv -> marker3
        T_cam_opencv_to_marker3 = np.eye(4)
        T_cam_opencv_to_marker3[:3, :3] = R_cam_opencv_to_marker3
        T_cam_opencv_to_marker3[:3, 3] = tvec_cam_opencv_to_marker3.flatten()
        
        rospy.loginfo("T_cam_opencv_to_marker3:\n" + str(T_cam_opencv_to_marker3))
        
        # Create homogeneous transformation matrix: robot_base -> marker3 (in user frame)
        T_robot_base_to_marker3_user_frame = np.eye(4)
        T_robot_base_to_marker3_user_frame[:3, :3] = np.eye(3)  # No rotation
        T_robot_base_to_marker3_user_frame[:3, 3] = self.t_robot_base_to_marker3_user_frame
        
        rospy.loginfo("T_robot_base_to_marker3_user_frame:\n" + str(T_robot_base_to_marker3_user_frame))
        
        # Create transformation matrix: cam_user_frame <- cam_opencv_frame
        T_cam_user_from_cam_opencv = np.eye(4)
        T_cam_user_from_cam_opencv[:3, :3] = self.R_opencv_to_user_frame
        
        rospy.loginfo("T_cam_user_from_cam_opencv:\n" + str(T_cam_user_from_cam_opencv))
        
        # Calculate transformation: robot_base -> cam_user_frame
        T_cam_opencv_to_marker3_inv = np.linalg.inv(T_cam_opencv_to_marker3)
        T_cam_user_from_cam_opencv_inv = np.linalg.inv(T_cam_user_from_cam_opencv)
        
        T_robot_base_to_cam_user = (
            T_robot_base_to_marker3_user_frame @
            T_cam_opencv_to_marker3_inv @
            T_cam_user_from_cam_opencv_inv
        )
        
        rospy.loginfo("T_robot_base_to_cam_user:\n" + str(T_robot_base_to_cam_user))
        
        # Get pose of target marker (ID 8) in OpenCV camera frame
        rvec_cam_opencv_to_marker15 = rvecs[8]
        tvec_cam_opencv_to_marker15 = tvecs[8]
        
        # Convert rotation vector to rotation matrix
        R_cam_opencv_to_marker15, _ = cv2.Rodrigues(rvec_cam_opencv_to_marker15)
        
        # Create homogeneous transformation matrix: camera_opencv -> marker15
        T_cam_opencv_to_marker15 = np.eye(4)
        T_cam_opencv_to_marker15[:3, :3] = R_cam_opencv_to_marker15
        T_cam_opencv_to_marker15[:3, 3] = tvec_cam_opencv_to_marker15.flatten()
        
        # Transform to user camera frame
        T_cam_user_to_marker15 = T_cam_user_from_cam_opencv @ T_cam_opencv_to_marker15
        
        # Transform to robot base frame
        T_robot_base_to_marker15 = T_robot_base_to_cam_user @ T_cam_user_to_marker15
        
        # Extract position
        t_robot_base_to_marker15 = T_robot_base_to_marker15[:3, 3]
        
        rospy.loginfo(f"Target marker position relative to robot base (meters):")
        rospy.loginfo(f"X: {t_robot_base_to_marker15[0]:.3f}")
        rospy.loginfo(f"Y: {t_robot_base_to_marker15[1]:.3f}")
        rospy.loginfo(f"Z: {t_robot_base_to_marker15[2]:.3f}")
        
        return t_robot_base_to_marker15[0], t_robot_base_to_marker15[1], t_robot_base_to_marker15[2]

    def go_to_joint_state(self):
        # Copy class variables to local variables to make the web tutorials more clear.
        # In practice, you should use the class variables directly unless you have a good
        # reason not to.
        move_group = self.move_group

        ## BEGIN_SUB_TUTORIAL plan_to_joint_state
        ##
        ## Planning to a Joint Goal
        ## ^^^^^^^^^^^^^^^^^^^^^^^^
        ## The Panda's zero configuration is at a `singularity <https://www.quora.com/Robotics-What-is-meant-by-kinematic-singularity>`_, so the first
        ## thing we want to do is move it to a slightly better configuration.
        ## We use the constant `tau = 2*pi <https://en.wikipedia.org/wiki/Turn_(angle)#Tau_proposals>`_ for convenience:
        # We get the joint values from the group and change some of the values:
        joint_goal = move_group.get_current_joint_values()
        joint_goal[0] = 0
        joint_goal[1] = -tau / 8
        joint_goal[2] = 0
        joint_goal[3] = -tau / 4
        joint_goal[4] = 0
        joint_goal[5] = tau / 6  # 1/6 of a turn
        # joint_goal[6] = 0

        # The go command can be called with joint values, poses, or without any
        # parameters if you have already set the pose or joint target for the group
        move_group.go(joint_goal, wait=True)

        # Calling ``stop()`` ensures that there is no residual movement
        move_group.stop()

        ## END_SUB_TUTORIAL

        # For testing:
        current_joints = move_group.get_current_joint_values()
        return all_close(joint_goal, current_joints, 0.01)

    def go_to_pose_goal(self, pose_goal):
        # Copy class variables to local variables to make the web tutorials more clear.
        # In practice, you should use the class variables directly unless you have a good
        # reason not to.
        move_group = self.move_group

        ## BEGIN_SUB_TUTORIAL plan_to_pose
        ##
        ## Planning to a Pose Goal
        ## ^^^^^^^^^^^^^^^^^^^^^^^
        ## We can plan a motion for this group to a desired pose for the
        ## end-effector:
        move_group.set_pose_target(pose_goal)

        ## Now, we call the planner to compute the plan and execute it.
        # `go()` returns a boolean indicating whether the planning and execution was successful.
        success = move_group.go(wait=True)
        # Calling `stop()` ensures that there is no residual movement
        move_group.stop()
        # It is always good to clear your targets after planning with poses.
        # Note: there is no equivalent function for clear_joint_value_targets().
        move_group.clear_pose_targets()

        ## END_SUB_TUTORIAL

        # For testing:
        # Note that since this section of code will not be included in the tutorials
        # we use the class variable rather than the copied state variable
        current_pose = self.move_group.get_current_pose().pose
        return all_close(pose_goal, current_pose, 0.01)

    def wait_for_state_update(
        self, box_is_known=False, box_is_attached=False, timeout=4
    ):
        # Copy class variables to local variables to make the web tutorials more clear.
        # In practice, you should use the class variables directly unless you have a good
        # reason not to.
        box_name = self.box_name
        scene = self.scene

        ## BEGIN_SUB_TUTORIAL wait_for_scene_update
        ##
        ## Ensuring Collision Updates Are Received
        ## ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ## If the Python node was just created (https://github.com/ros/ros_comm/issues/176),
        ## or dies before actually publishing the scene update message, the message
        ## could get lost and the box will not appear. To ensure that the updates are
        ## made, we wait until we see the changes reflected in the
        ## ``get_attached_objects()`` and ``get_known_object_names()`` lists.
        ## For the purpose of this tutorial, we call this function after adding,
        ## removing, attaching or detaching an object in the planning scene. We then wait
        ## until the updates have been made or ``timeout`` seconds have passed.
        ## To avoid waiting for scene updates like this at all, initialize the
        ## planning scene interface with  ``synchronous = True``.
        start = rospy.get_time()
        seconds = rospy.get_time()
        while (seconds - start < timeout) and not rospy.is_shutdown():
            # Test if the box is in attached objects
            attached_objects = scene.get_attached_objects([box_name])
            is_attached = len(attached_objects.keys()) > 0

            # Test if the box is in the scene.
            # Note that attaching the box will remove it from known_objects
            is_known = box_name in scene.get_known_object_names()

            # Test if we are in the expected state
            if (box_is_attached == is_attached) and (box_is_known == is_known):
                return True

            # Sleep so that we give other threads time on the processor
            rospy.sleep(0.1)
            seconds = rospy.get_time()

        # If we exited the while loop without returning then we timed out
        return False
        ## END_SUB_TUTORIAL

    def add_scene_constraints(self, timeout=4):
        # Copy class variables to local variables to make the web tutorials more clear.
        # In practice, you should use the class variables directly unless you have a good
        # reason not to.
        box_name =  "box"
        scene = self.scene

        ## BEGIN_SUB_TUTORIAL add_box
        ##
        ## Adding Objects to the Planning Scene
        ## ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        ## First, we will create a box in the planning scene between the fingers:
        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = -0.32
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = 0.5  # above the panda_hand frame
        box_name = "back_wall"
        scene.add_box(box_name, box_pose, size=(0.2, 1.3, 1.3))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0
        box_pose.pose.position.y = -0.665
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = 0.5  
        box_name = "left_wall"
        scene.add_box(box_name, box_pose, size=(1.3, 0.15, 1.3))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0
        box_pose.pose.position.y = 0.4
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = 0.5  
        box_name = "right_wall"
        scene.add_box(box_name, box_pose, size=(1.3, 0.2, 1.3))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))


        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0.08
        box_pose.pose.position.y = 0
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = 1.0  
        box_name = "up_wall"
        scene.add_box(box_name, box_pose, size=(1.2, 1.2, 0.2))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0.63
        box_pose.pose.position.y = 0.0
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = 0.35  
        box_name = "forward_wall"
        scene.add_box(box_name, box_pose, size=(0.2, 1.0, 2.0))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0
        box_pose.pose.position.y = 0
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = -0.33  
        box_name = "down_wall"
        scene.add_box(box_name, box_pose, size=(0.7, 0.7, 0.65))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

        box_pose = geometry_msgs.msg.PoseStamped()
        box_pose.header.frame_id = "world"
        box_pose.pose.position.x = 0
        box_pose.pose.position.y = -0.525
        box_pose.pose.orientation.w = 0.0
        box_pose.pose.position.z = -0.27 
        box_name = "conveyor_wall"
        scene.add_box(box_name, box_pose, size=(1.5, 0.35, 0.65))
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))
        ## END_SUB_TUTORIAL
        # Copy local variables back to class variables. In practice, you should use the class
        # variables directly unless you have a good reason not to.
        self.box_name = box_name
        print(self.wait_for_state_update(box_is_known=True, timeout=timeout))

    def display_trajectory(self, plan):
        robot = self.robot
        display_trajectory_publisher = self.display_trajectory_publisher

        display_trajectory = moveit_msgs.msg.DisplayTrajectory()
        display_trajectory.trajectory_start = robot.get_current_state()
        display_trajectory.trajectory.append(plan)
        # Publish
        display_trajectory_publisher.publish(display_trajectory)

    def execute_plan(self, plan):
        # Copy class variables to local variables to make the web tutorials more clear.
        # In practice, you should use the class variables directly unless you have a good
        # reason not to.
        move_group = self.move_group

        ## BEGIN_SUB_TUTORIAL execute_plan
        ##
        ## Executing a Plan
        ## ^^^^^^^^^^^^^^^^
        ## Use execute if you would like the robot to follow
        ## the plan that has already been computed:
        move_group.execute(plan, wait=True)

        ## **Note:** The robot's current joint state must be within some tolerance of the
        ## first waypoint in the `RobotTrajectory`_ or ``execute()`` will fail
        ## END_SUB_TUTORIAL

    def go_conveyor_up(self):
        """
        position: 
            x: -4.765560879524561e-06
            y: -0.49290802215605667
            z: 0.4317289625723313
        orientation: 
            x: 2.061813753460185e-06
            y: -0.999999996977919
            z: 6.80678734571758e-05
            w: 3.750567278336623e-05
        """
        pose_goal = geometry_msgs.msg.Pose()
        pose_goal.position.x = -4.765560879524561e-06
        pose_goal.position.y = -0.49290802215605667
        pose_goal.position.z = 0.4317289625723313

        pose_goal.orientation.x = 2.061813753460185e-06
        pose_goal.orientation.y = 0.999999996977919
        pose_goal.orientation.z = 6.80678734571758e-05
        pose_goal.orientation.w = 3.750567278336623e-05

        self.go_to_pose_goal(pose_goal)
    
    def go_home(self):
        """
        position: 
            x: 0.2800004276046158
            y: -6.195926307662706e-07
            z: 0.5499995462155063
        orientation: 
            x: 8.726608182759839e-07
            y: 0.9999999999958115
            z: -8.726669106190632e-07
            w: 2.6179931165000596e-06
        """
        pose_goal = geometry_msgs.msg.Pose()
        pose_goal.position.x = 0.2800004276046158
        pose_goal.position.y = -6.195926307662706e-07
        pose_goal.position.z = 0.5499995462155063

        pose_goal.orientation.x = 8.726608182759839e-07
        pose_goal.orientation.y = 0.9999999999958115
        pose_goal.orientation.z = -8.726669106190632e-07
        pose_goal.orientation.w = 2.6179931165000596e-06

        self.go_to_pose_goal(pose_goal)

    def go_base(self):
        """
        position: 
            x: 0.021514432545543048
            y: -0.22226675350695183
            z: 0.2006470634049244
        orientation: 
            x: 3.396026614338458e-05
            y: -0.9999999987625146
            z: 3.623433423885886e-05
            w: 2.9570671063740035e-06
        """
        pose_goal = geometry_msgs.msg.Pose()
        # self.base_frame = [22.1433449, -230.209839, -56.431284]
        pose_goal.position.x = 0.021514432545543048
        pose_goal.position.y = -0.22226675350695183
        pose_goal.position.z = 0.5499995462155063 

        pose_goal.orientation.x = 3.396026614338458e-05
        pose_goal.orientation.y = -0.9999999987625146
        pose_goal.orientation.z = 3.623433423885886e-05
        pose_goal.orientation.w = 2.9570671063740035e-06

        self.go_to_pose_goal(pose_goal)

    def open_gripper(self):
        self.gripper_srv(True, False)

    def close_gripper(self):
        self.gripper_srv(False, True)

def main():
    
    try:
        print("")
        print("----------------------------------------------------------")
        print("Welcome to the MoveIt MoveGroup Python Drawing Interface")
        print("----------------------------------------------------------")
        print("Press Ctrl-D to exit at any time")
        print("")
        input(
            "============ Press `Enter` to begin program by setting up the moveit_commander ..."
        )
        move_group = MoveGroupPythonInterface()
        
        # Открытие захвата(реализован через ros-сервис)
        move_group.open_gripper()
        
        # Переход в home позицию
        move_group.go_home()
        
        # Добавление виртуальных стен в сцену робота
        #move_group.add_scene_constraints()

        # Получение актуальной позиции tool0 - точка куда установлен захват - НЕ НИЖНЯЯ ЧАСТЬ ЗАХВАТА
        cur_pose=move_group.move_group.get_current_pose().pose
        print(cur_pose)

        print("Обнаружение маркеров ArUco...")
        x_pos, y_pos, z_pos = move_group.get_marker_position()
        #print ("x = {x_pos}, y= {y_pos}, z = {z_pos}")

        # Переход в тоску над конвейером
        move_group.go_conveyor_up()
       
        input(
            "============ Подход к точке на конвейере ============")
        
        # Получаем координаты маркера с помощью камеры
        print("Ожидание изображения с камеры...")
        rospy.sleep(2.0)  # Даем время на получение изображения
        

        
        if x_pos is None or y_pos is None or z_pos is None:
            print("Не удалось определить позицию маркера. Используем значения по умолчанию.")
            x_pos = 0.02262
            y_pos = -0.52124
            z_pos = 0.257
        else:
            print(f"Найдена позиция маркера: X={x_pos:.4f}, Y={y_pos:.4f}, Z={z_pos:.4f}")
            # Преобразуем из метров в координаты для робота (если необходимо)
            # В данном случае значения уже в метрах, что соответствует системе координат робота
            
        # Создаем целевую позу
        pose_goal = move_group.move_group.get_current_pose().pose
        pose_goal.position.x = x_pos 
        pose_goal.position.y = y_pos
        pose_goal.position.z = 0.257 # Минимальный Z, чтобы не врезаться в конвейер
        move_group.go_to_pose_goal(pose_goal)
        
        print(pose_goal)
        
        #Закрытие гриппера
        move_group.close_gripper()
        cur_pose=move_group.move_group.get_current_pose().pose
        print(cur_pose)
        
        input(
            "============ Возращение в home ============")
        cur_pose=move_group.move_group.get_current_pose().pose
        print(cur_pose)

        move_group.go_conveyor_up()
        
        move_group.go_home()
        
        move_group.open_gripper()

    except rospy.ROSInterruptException:
        return
    except KeyboardInterrupt:
        return


if __name__ == "__main__":
    main()